---
layout: post
title: Presidential Speech Analysis
---
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 90%;
}
</style>

In my most recent project I utilized NLP techniques to analyze a [corpus of presidential speeches](http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus#) from George Washington to Donald Trump, a total of 1,044 speeches. The focus of this project was on discovering and tracking topics, or themes, across these speeches.

## Workflow

First, I loaded all of the speeches into MongoDB on an AWS EC2 instance, just to keep my MongoDB and AWS skills fresh. Each document was cleaned with regex to remove punctuation, quotes, text in brackets, etc. I used [spaCy](https://spacy.io/) to do some initial processing on the documents, including tokenization, lemmatization, and named entity removal. spaCy's pipeline method made this easy. Next, sklearn's count vectorizer was used to transform the documents into a machine-readable format for topic modeling. I tried a TFIDF count vectorizer as well, but found it made the topic models pick up on obscure topics rather than more general themes.

I took two approaches to topic modeling, each with a seperate goal. First, I used Gensim's latent Dirichlet allocation (**LDA**) implementation as a method of topic discovery. LDA is an unsupervised model that, in this case, assumes each speech contains a small number of topics. LDA automatically assigns words to specific topics based on their co-occurence, and then looks at the words of each speech to assign one or more topics a speech. Each topic is characterized by its most frequent words, which the user inspects to determine what the topic is about. I used [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html) to visualize the the results.

In the second topic modeling approach I used [**CorEx**](https://github.com/gregversteeg/corex_topic) to "track" topics across the speeches. CorEx is a semi-supervised topic model which takes seed words as input for each topic, and forms the topics around those seed words. Basically, it does the same thing as LDA, except you can give the model hints on what topics you want to look for. I used CorEx to look at seven common political themes today: immigration, health care, national security, economy, trade, education, and conflict. To seed these topics, I used the following words: [immigration, border], [health, care], [national, security], [economy], [trade], [education], and [war, military].



## Results

#### LDA

The discovered topics were visualized with pyLDAvis. In the image below, the graph on the right shows each topic as a circle, with the size of the circle corresponding to the number of speeches that included that topic. The largest topic is highlighted. On the right, the bar chart shows the most frequent words from the highlighted topic. This topic seems to be about war. I found that LDA discovered fairly coherent topics, if a little broad/ vague.

![pyldavis]({{ site.url }}/images/p4_pyldavis.png)


#### CorEx

CorEx was excellent at forming topics around the seed words I provided. For example, with just the seed word "economy", CorEx found a topic with the following most frequent words: "economy", "budget", "economic", "growth", "investment", "market", "reduce", "progress", "continue", and "development". This is exactly what I was looking for. I used this model to visualize the percentage of each president's speeches that included a topic.

Note that Trump's speeches are from the campaign trail, not his time in office.

![topics]({{ site.url }}/images/p4_topics_by_pres.png)


## Summing up

Unsupervised modeling was pretty good at discovering topics, while the semi-supervised approach was great at tracking specific topics. Unsupervised and semi-supervised learning is much preferable to supervised learning because it doesn't require labeled data, but is only applicable to certain problems. Topic modeling is one of those problems.

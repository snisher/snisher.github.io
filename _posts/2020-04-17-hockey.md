---
layout: post
title: Fantasy Hockey Player Value Prediction
---
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 90%;
}
</style>

## Intro and Data
In fantasy sports, there are multiple draft styles, one of them being an auction draft. Auction style drafts work as you might expect, with a player put up for auction going to the highest bidder. Each person in the league starts with a limited number of fantasy dollars to fill out their team. For people who like to plan excessively for their draft, this poses a problem: How do you effectively budget your fantasy money when you don't know how much each player will go for in the draft? I tried to tackle this problem for fantasy hockey by predicting the average value of each player in an auction draft.

![bid]({{ site.url }}/images/p2_auction.jpg)

Specifically, I trained a linear regression model to predict the [2019 average auction draft values](https://hockey.fantasysports.yahoo.com/hockey/draftanalysis?tab=AD&pos=ALL&sort=DA_AP) released by Yahoo. This data is the average dollar value of each player across all leagues that had an auction style draft. To predict the average value, I used [2018 player stats](https://hockey.fantasysports.yahoo.com/hockey/29483/players?&sort=AR&sdir=1&status=ALL&pos=P&stat1=S_S_2018&jsenabled=1), as well as [theoretical player values](https://www.rotowire.com/hockey/auction-values.php).

I want to talk a bit about including the theoretical player value as a feature for this model, because I know what you might be thinking: "using the theoretical player value to predict the average player value? That's cheating!" To which I would say: It's not cheating, because the theoretical value for each player is a **linear combination** of the other stats that are already provided to the model. The reason I include it is because it helps the model to have it as a given feature rather than training it to calculate the value. Furthermore, the theoretical value is actually **not that similar** to the average value. The highest theoretical value is $38, while the highest average value is $62.5. Half of the theoretical values are negative, while all average values are positive. So the theoretical values are correlated, but different.

## The model
Scikit Learn has a very solid machine learning framework which I used to implement linear regression. I played around with different regularization techniques and feature manipulation, eventually deciding to use lasso (L1) regularization and polynomial features. I'll share the code for the predictions below.

First the features were transformed into polynomial features. Using polynomial features helps to model non-linear curves.
```
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.fit_transform(X_test)
```

Then the polynomial features were scaled to have a mean of 0 and a standard deviation of 1.
```
scaler = StandardScaler().fit(X_train_poly)
X_train_poly = scaler.transform(X_train_poly)
X_test_poly = scaler.transform(X_test_poly)
```

Next a linear regression model with lasso regularization was fit to the training data. The alpha value, which scales the strength of the regularization penalty, was chosen out of the provided values using cross validation.
```
alphavec = np.linspace(0.01,1,200) # list of alpha values to try

model = LassoCV(alphas=alphavec, cv=5)
model.fit(X_train_poly, y_train)
```

Finally, I made predictions on the unseen test set to get an idea of the model's performance.
```
preds = model.predict(X_test_poly)
print("R^2: ", metrics.r2_score(y_test, preds))
print("MAE: ", metrics.mean_absolute_error(y_test, preds))
```

The model had an **R<sup>2</sup>** score of **0.748**, and a **mean absolute error** (MAE) score of **5.39**. R<sup>2</sup> scores provide a rough estimate of how much variance in the data is accounted for by the model, in this case about 75% of the variance. MAE is more easily interpretable because it has the same units as the value that was predicted. In this case, the model had an MAE of 5.39, which means that on average the model's prediction was $5.39 off from the actual average value. That's pretty good when taking into account the the maximum average value was $62.

Knowing approximately how much money each player will go for allows anyone to make a detailed budget for their auction draft. By plugging in the 2019 season player stats, the model will make actionable predictions for the 2020 draft!

Here's a pretty graph showing the predicted vs actual values.

![pred vs actual]({{ site.url }}/images/p2_pred_vs_avg_value.png)

<br/>

Note the pile up of dots in the bottom left. Those represent players who were drafted for a very small value, most likely at the end of the draft, when no-one has any money left. The model does a poor job of predicting these players' values because their average draft value doesn't reflect their stats as much as it reflects people not having any money at the end of the draft.

## Future work

This model was trained only on data from the 2019 draft (~150 data points) and could benefit greatly from seeing more data. Given enough data I would consider removing all players with average draft values less than $3 or $4, because, like I said above, these values don't reflect player stats as much as they reflect people not having enough money at the end of the draft. You can see the distribution of average player values below.

![avg val]({{ site.url }}/images/p2_avg_val_hist.png)
